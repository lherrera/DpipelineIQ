{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f3a373-0101-4617-a5c8-19b6bfea3749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Load the data and select the relevant column, dropping nulls\n",
    "df = spark.table(\"users.luis_herrera.pipelineiq_view\").select(\"business_usecase_type\").na.drop()\n",
    "\n",
    "# Tokenize the business use case type into words\n",
    "tokenizer = Tokenizer(inputCol=\"business_usecase_type\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(df)\n",
    "\n",
    "# Vectorize the tokenized words using HashingTF\n",
    "vectorizer = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=1000)\n",
    "vectorized_data = vectorizer.transform(words_data)\n",
    "\n",
    "# Evaluate silhouette scores for k in range 2 to 15\n",
    "scores = []\n",
    "for k in range(2, 16):\n",
    "    # Fit KMeans model for current k\n",
    "    kmeans = KMeans(featuresCol=\"features\", k=k, seed=42)\n",
    "    kmeans_model = kmeans.fit(vectorized_data)\n",
    "    # Assign clusters to data\n",
    "    clusters = kmeans_model.transform(vectorized_data)\n",
    "    # Evaluate clustering using silhouette score\n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"features\")\n",
    "    score = evaluator.evaluate(clusters)\n",
    "    scores.append((k, score))\n",
    "\n",
    "# Create a DataFrame of silhouette scores for each k and display\n",
    "scores_df = spark.createDataFrame(scores, [\"k\", \"silhouette_score\"])\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9691eff-e1f1-46f2-8463-d13ec30c7cb1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762325020857}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"category_name\":420},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762325344692}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load the data and select the relevant column, dropping nulls\n",
    "df = spark.table(\"users.luis_herrera.pipelineiq_view\").select(\"business_usecase_type\").na.drop()\n",
    "\n",
    "# Tokenize the business use case type into words\n",
    "tokenizer = Tokenizer(inputCol=\"business_usecase_type\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(df)\n",
    "\n",
    "# Vectorize the tokenized words using HashingTF\n",
    "vectorizer = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=1000)\n",
    "vectorized_data = vectorizer.transform(words_data)\n",
    "\n",
    "# Fit KMeans model with k=16 clusters\n",
    "kmeans = KMeans(featuresCol=\"features\", k=16, seed=42)\n",
    "kmeans_model = kmeans.fit(vectorized_data)\n",
    "# Assign clusters to data\n",
    "clusters = kmeans_model.transform(vectorized_data)\n",
    "\n",
    "# Get top 5 words per cluster to use as category names\n",
    "top_words = (\n",
    "    clusters\n",
    "    .select(\"prediction\", F.explode(\"words\").alias(\"word\"))\n",
    "    .groupBy(\"prediction\", \"word\")\n",
    "    .count()\n",
    "    .withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"prediction\").orderBy(F.desc(\"count\"))))\n",
    "    .filter(F.col(\"rank\") <= 5)\n",
    "    .groupBy(\"prediction\")\n",
    "    .agg(F.collect_list(F.col(\"word\")).alias(\"top_words\"))\n",
    "    .withColumn(\"category_name\", F.expr(\"concat_ws(' ', slice(top_words, 1, 5))\"))\n",
    "    .select(\"prediction\", \"category_name\")\n",
    ")\n",
    "\n",
    "# Count the number of use cases per cluster/category\n",
    "category_counts = (\n",
    "    clusters.groupBy(\"prediction\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"usecase_count\")\n",
    ")\n",
    "\n",
    "# Join top words and counts for each cluster\n",
    "top_words_with_counts = top_words.join(category_counts, on=\"prediction\", how=\"left\")\n",
    "\n",
    "# Add category names and counts to the original clustered data\n",
    "clusters_with_names = clusters.join(top_words_with_counts, on=\"prediction\", how=\"left\")\n",
    "\n",
    "# Display the clustered data with category names\n",
    "display(clusters_with_names.select(\"business_usecase_type\", \"prediction\", \"category_name\"))\n",
    "\n",
    "# Display the top words and use case counts for each cluster\n",
    "display(top_words_with_counts.orderBy(\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94bbb4ee-696d-41d5-9157-5b13ad69bf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(category_names)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Technical coverage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
